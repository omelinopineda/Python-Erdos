{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Clustering Algorithms\n",
    "\n",
    "Recall the concentric circle toy data that we studied last time, which we recreate below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X1, y1 = make_circles(n_samples=500, noise = 0.02, random_state = 3)\n",
    "plt.scatter(X1[:,0],X1[:,1],c=y1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that $k$-Means did a terrible job at clustering the concentric circles data. This also makes sense intuitively: the mean of each ring is roughly at the origin! Let's double check this by running $k$-Means clustering with $k=2$. \n",
    "\n",
    "We could use the function in `scikit-learn`, but for fun let's use the $k$-Means algorithm that we wrote previously. The issue is that our algorithm was written in a different notebook! It would be annoying if we had to search through the other notebook to copy and paste the necessary functions into this one. We'll take this opportunity to see how to load in a function from a `.py` file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Our $k$-Means Function\n",
    "\n",
    "If you look in the directory containing this notebook, there is a folder called \"code\". Inside is a file called `kmeans.py`. Go ahead and take a look at it now...\n",
    "\n",
    "You should see that it contains exactly the $k$-Means functions that we wrote last time. We can load these functions into the notebook as follows. Note that certain terminal commands work in Jupyter notebooks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cd code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are in the `code` directory, we can load in our `kMeans` function from the python file `kmeans.py` using standard syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kmeans import kMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to our original directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see if `kMeans` will run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers, labels = kMeans(X1,2)\n",
    "\n",
    "plt.scatter(X1[:,0],X1[:,1],c=labels)\n",
    "plt.scatter(centers[:,0],centers[:,1],marker = '^')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, the function works like it did yesterday (i.e., gives a bad result!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also import the whole package and use any function in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kmeans as km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.cluster_centers(X1,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final note on this theme, if you want to do more involved stuff with different directories or if you are working with Python in the terminal, you should import the module `os`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the package\n",
    "import os\n",
    "\n",
    "# Example function - 'get current working directory'\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other useful functions:\n",
    "\n",
    "`os.chdir('path')` changes current current working directory to `path`\n",
    "\n",
    "`os.listdir()` lists files and subdirectories in the current directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN\n",
    "\n",
    "Our goal is to find a clustering algorithm which will cluster the concentric circles example 'correctly'. One candidate is *DBSCAN (Density-based spatial clustering of applications with noise)*. \n",
    "\n",
    "The algorithm is described as follows.\n",
    "\n",
    "#### Input Data\n",
    "\n",
    "We begin with a dataset $X = \\{\\vec{x}_1,\\ldots,\\vec{x}_n\\}$, each $\\vec{x}_j \\in \\mathbb{R}^d$, which we would like to cluster. \n",
    "\n",
    "#### Parameter Selection\n",
    "\n",
    "Like $k$-Means, there is some parameter selection involved in DBSCAN. Here we choose a small radius $\\epsilon > 0$ and a positive integer `minPoints`.\n",
    "\n",
    "#### The Algorithm\n",
    "\n",
    "We say $\\vec{x} \\in X$ is a *core point* if there are `minPoints` elements of $X$ within distance $\\epsilon$ from it.\n",
    "\n",
    "Another $\\vec{y} \\in X$ is said to be *reachable* from a core point $\\vec{x}$ if there is a chain of core points $\\vec{x} = \\vec{x}_{j_1}, \\vec{x}_{j_2}, \\ldots, \\vec{x}_{j_\\ell} = \\vec{y}$ such that the distance between consective linkes in the chain is less than $\\epsilon$. \n",
    "\n",
    "Each cluster of DBSCAN contains a core point and all points which are reachable from it.\n",
    "\n",
    "**Remarks:** \n",
    "- The number of clusters is not set beforehand. Rather, it is determined by the data.\n",
    "- Some datapoints may not belong to any cluster; these are called *noise points*. \n",
    "- There is a need to tune parameters $\\epsilon$ and `minPoints` (especially $\\epsilon$!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it on some new toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "count = 1000\n",
    "angles = np.random.uniform( 0, 2*np.pi, count )\n",
    "ring = np.array([np.sin( angles ), np.cos( angles )]) * np.random.uniform(2,3,count)\n",
    "ring = ring.transpose()\n",
    "blob = np.random.normal(0,0.3,size=(count,2))\n",
    "\n",
    "y = np.repeat([0,1],count)\n",
    "X = np.vstack( [ring,blob] )\n",
    "\n",
    "plt.scatter( X[:,0], X[:,1],c=y )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's try clustering with $k$-Means. By its nature, we should expect that it doesn't work too well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=2).fit(X)\n",
    "plt.scatter( X[:,0], X[:,1],c=kmeans.labels_ )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our expectation panned out. Now let's try DBSCAN, which is included in `scikit-learn`. If we don't specify parameters, $\\epsilon$ and `minPoints` are set to default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "dbscan = DBSCAN().fit(X)\n",
    "plt.scatter( X[:,0], X[:,1],c=dbscan.labels_ )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "1) Try running `DBSCAN` on the concentric circles dataset `X1` from above. How well does it do?\n",
    "\n",
    "2) `DBSCAN` has lots of options; dig through the documentation to see if you can get it to cluster `X1` correctly https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Experiment with DBSCAN on the datasets `X2`, `X3` and `X4` defined below. In particular, try using different values of `eps`. What happens if you make `eps` super small? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X2, y2 = make_blobs(n_samples=500, center_box=(-3,3), centers=3, random_state=6)\n",
    "\n",
    "\n",
    "X3, y3 = make_blobs(n_samples=1000, centers=4, random_state=1)\n",
    "\n",
    "xs = np.linspace(0,2*np.pi,500)\n",
    "X4 = np.array([np.cos(xs),np.sin(xs)]).T\n",
    "\n",
    "from sklearn.datasets import make_circles\n",
    "X5 = np.append(X4, np.random.multivariate_normal(np.array([0,0]), 0.5*np.array([[1,0],[0,1]]), 50), axis=0)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "p1 = fig.add_subplot(2,2,1)\n",
    "p1.scatter(X2[:,0],X2[:,1])\n",
    "plt.title('Example 2')\n",
    "\n",
    "p2 = fig.add_subplot(2,2,2)\n",
    "p2.scatter(X3[:,0],X3[:,1])\n",
    "plt.title('Example 3')\n",
    "\n",
    "p3 = fig.add_subplot(2,2,3)\n",
    "p3.scatter(X4[:,0],X4[:,1])\n",
    "plt.title('Example 4');\n",
    "\n",
    "p4 = fig.add_subplot(2,2,4)\n",
    "p4.scatter(X5[:,0],X5[:,1])\n",
    "plt.axis('equal')\n",
    "plt.title('Example 5');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing $\\epsilon$\n",
    "\n",
    "We are now faced with the question of how to choose $\\epsilon$ and `minPoints` in an unsupervised context (where we can't just *look* at the data). As with choosing $k$ in the $k$-Means algorithm, this is not an exact science. We can once again try to do an \"elbow analysis\". Suppose we have fixed `minPoints` (heuristically, this is the minimum size of cluster that we will accept). It is suggested in the literature to look for an elbow in the histogram of $k$NN distances, with $k =$ `minPoints`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Compute kNN distances\n",
    "nbrs = NearestNeighbors(n_neighbors=5).fit(X2)\n",
    "distances, indices = nbrs.kneighbors(X2)\n",
    "\n",
    "# Look at the last column of distances\n",
    "eps_dist = distances[:,-1]\n",
    "\n",
    "# Plot the histogram\n",
    "plt.hist(eps_dist,bins=50)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we tried to do something like DBSCAN, but didn't fix an $\\epsilon$ parameter? Instead, we could look at all values of $\\epsilon$ and see how the clustering behavior changes over them. This is the perspective of *hierarchical clustering*, where the data isn't partitioned into one fixed partition, but is instead clustered in a 'multiscale' fashion. Basically, we start with the discrete data set partitioned into singleton sets, then join partition sets over time to form coarser and coarser clusters.\n",
    "\n",
    "Let's take a look at a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[0,3],[0.5,2.5],[2,1],[1.5,1.2],[2.1,0.9],[5,3],[5.5,2.8]])\n",
    "y = ['a','b','c','d','e','f','g']\n",
    "plt.scatter(X[:,0],X[:,1])\n",
    "for i in range(7):\n",
    "    plt.annotate(y[i], (X[i,0], X[i,1]))\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform hierarchical clustering using the `linkage` function. The result can be displayed as a visualization called a *dendrogram*. Note the function comes from another package called `scipy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage  \n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "linked = linkage(X)\n",
    "dendrogram(linked, labels=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, this should look like a reasonable visual description of the multiscale clustering of the data.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "\n",
    "This version of a dendrogram is described by the following rule. Points $\\vec{x}$ and $\\vec{y}$ in our dataset are joined in the dendrogram below height $\\epsilon$ if and only if there is a chain of data points $\\vec{x} = \\vec{x}_1,\\vec{x}_2,\\ldots,\\vec{x}_n = \\vec{y}$ such that each consective pair of links $\\vec{x}_i$ and $\\vec{x}_{i+1}$ is within distance $\\epsilon$. \n",
    "\n",
    "There are several related algorithms for building dendrograms. One great thing about hierarchical clustering is that it is very flexible; in fact, the algorithm outlined above makes sense in general metric spaces.\n",
    "\n",
    "If we want to get really fancy: the map taking a dataset to its dendrogram is a functor from the category of finite metric spaces to the category of finite ultrametric spaces which is stable with respect to Gromov-Hausdorff distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Examples\n",
    "\n",
    "Let's try it on our 'concentric circles' data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "linked = linkage(X1)\n",
    "dendrogram(linked, labels=y1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like two clusters! But are they really corresponding to the two circles? We can apply `AgglomerativeClustering` from `scikit-learn` to extract the points in the two big clusters. This clustering algorithm requires a choice of number of clusters `n_clusters`, then it slices the dendrogram into the `n_clusters` most prominent clusters. The benefit here is that the choice of `n_clusters` is much more clear than in the $k$-Means algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "ac = AgglomerativeClustering(n_clusters=2, linkage = 'single')\n",
    "ac.fit(X1)\n",
    "ac.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X1[:,0],X1[:,1],c=ac.labels_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked! Interestingly, if we increase the number of clusters the algorithm picks out the 'connected pieces' of the circles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = AgglomerativeClustering(n_clusters=7, linkage = 'single')\n",
    "ac.fit(X1)\n",
    "plt.scatter(X1[:,0],X1[:,1],c=ac.labels_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of options for hierarchical clustering. In particular, dendrograms can be created via different algorithms; see https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.cluster.hierarchy.linkage.html. Similar options are available for Agglomerative clustering https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html. Performance of each method will depend on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "linked = linkage(X1, 'ward')\n",
    "dendrogram(linked, labels=y1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ac = AgglomerativeClustering(n_clusters=2, linkage = 'ward')\n",
    "ac.fit(X1)\n",
    "plt.scatter(X1[:,0],X1[:,1],c=ac.labels_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** The default linkage type for `AgglomerativeClustering` is `ward`, which gives a bad result here. Playing with the options for any particular application is important!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering on MNIST\n",
    "\n",
    "Dendrogram visualizations tend to not be very readable for large numbers of datapoints. Let's look at the clustering behavior on a random collection of points in MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "MNIST, MNISTlabels = load_digits(return_X_y = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "r = np.random.choice(len(MNIST),size=100)\n",
    "smallMNIST = MNIST[r]\n",
    "smallMNISTlabels = MNISTlabels[r]\n",
    "\n",
    "linked = linkage(smallMNIST,'complete')\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "dendrogram(linked, ax=ax, labels=smallMNISTlabels)\n",
    "ax.tick_params(axis='x', which='major', labelsize=15, rotation=90)\n",
    "ax.tick_params(axis='y', which='major', labelsize=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could cluster all of the points in MNIST, but change display settings in the dendrogram. Available options can be seen here: https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.cluster.hierarchy.dendrogram.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_complete = linkage(MNIST,'complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_ward = linkage(MNIST,'ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_single = linkage(MNIST,'single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to make it easier to change display settings\n",
    "linkages = {'complete':linked_complete,'ward':linked_ward,'single':linked_single}\n",
    "\n",
    "linkage_type = 'complete'\n",
    "truncate_type = 'level'\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "dendrogram(linkages[linkage_type], ax=ax, labels=MNISTlabels, p=5, truncate_mode = truncate_type)\n",
    "ax.tick_params(axis='x', which='major', labelsize=15, rotation=90)\n",
    "ax.tick_params(axis='y', which='major', labelsize=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering on Stock Data\n",
    "\n",
    "Let's take a look at the hierarchical clustering behavior of some stock data. The code below shows how to read in multiple files from a folder into the same variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os, json\n",
    "# Create a list of file names for files in the stocks folder\n",
    "files = [f for f in os.listdir('data/stocks') if f.endswith('.json')]\n",
    "\n",
    "data = {} # Create an empty dictionary\n",
    "symbols = [] # Create an empty list\n",
    "for name in files:\n",
    "    with open(os.path.join('data/stocks/', name),'r') as f:\n",
    "        d = json.load(f) # Load each file\n",
    "        symbol = d['Meta Data']['2. Symbol'] # Pull off the stock symbol and add to the list\n",
    "        symbols.append( symbol )\n",
    "        data[ symbol ] = d['Time Series (Daily)'] # Get the time series for the symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are stocks for Apple, Amazon, Ford, Facebook, General Motors, Google, Honda, Netflix and Toyota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['AAPL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that `data` is a dictionary. Each key is a stock name, and the value for each key is recent daily stock data. Let's study the 'close' price as a time series. This is still pretty hard to parse, so let's convert it to a `pandas` data frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Start with a column containing dates\n",
    "market = pd.DataFrame(columns=['date'])\n",
    "\n",
    "for symbol in data.keys():\n",
    "    # Add dates in a standard format\n",
    "    dates = pd.DataFrame( pd.to_datetime( list(data[symbol].keys()) ), columns=['date'] )\n",
    "    # Pull off closing prices\n",
    "    closes = pd.DataFrame( [float(x['5. adjusted close']) for x in data[symbol].values()], columns=[symbol] )\n",
    "    # Create dataframe of dates and closes for the current symbol\n",
    "    df = pd.DataFrame( pd.concat( [dates, closes], axis=1 ) )\n",
    "    # Merge with the main dataframe\n",
    "    market = market.merge( df, how='outer' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a numpy array for computations. We don't really have a use for the dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "prices = market.drop('date',axis=1)\n",
    "symbols = list(prices.columns)\n",
    "X = np.array([prices[s] for s in symbols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prices.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the stock prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "\n",
    "for j in range(9):\n",
    "    fig.add_subplot(3,3,j+1)\n",
    "    plt.plot(X[j,:])\n",
    "    plt.title(list(data.keys())[j])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can cluster the stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "linked = linkage(X, 'complete')\n",
    "dendrogram(linked, labels=symbols)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is not too informative. We are computing distances between the time series by thinking of them as vectors in $\\mathbb{R}^{100}$. Notice that the stock prices differ by orders of magnitude. If we are intersted in 'trends' more than absolute dollar amount, we could try normalizing to the unit sphere in $\\mathbb{R}^{100}$. This is essentially accomplished if we use `metric = 'cosine'` in the linkage function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked = linkage(X, 'complete', metric = 'cosine')\n",
    "dendrogram(linked, labels=symbols)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results here seem to make sense! Strange that 'Ford' is not clustered with the other car manufacturers though..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaf Contours\n",
    "\n",
    "Let's load in a dataset containing 2D contours of many samples of leaves coming from 3 different species. The data is saved in a matlab file, so it will take a little bit of work to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from os.path import dirname, join as pjoin\n",
    "import scipy.io as sio\n",
    "\n",
    "mat_fname = 'data/leafContours.mat'\n",
    "# Add the file name to the current working directory.\n",
    "\n",
    "mat_contents = sio.loadmat(mat_fname) # Read the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what is contained in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we need to separate the actual data from the metadata. The types of data in the file are listed under several keys. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_contents.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plane curves we are after are under the 'planarShapes' key. Let's extract that from the mat file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leafShapes = mat_contents['leafContours']\n",
    "leafShapes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To match with our usual convention, we would prefer this to be transposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leafShapes = leafShapes.T\n",
    "leafShapes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second command above shows that `leafShapes` is a 2x100x179 array. Exploring more, we would find that there are 179 samples of leaves, each stored as a 2x100 pointcloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "leaves = [leafShapes[125,:,:]] # Pick some shapes.\n",
    "leaves.append(leafShapes[80,:,:])\n",
    "leaves.append(leafShapes[1,:,:])\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "for j in range(3):\n",
    "    fig.add_subplot(1,3,j+1)\n",
    "    plt.axis('equal')\n",
    "    plt.plot(leaves[j][:,0],leaves[j][:,1],linewidth=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should similarly get the labels from `leafLabels.mat`. The labels are by species of tree the leaf came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_fname = 'data/leafLabels.mat' \n",
    "\n",
    "mat_contents = sio.loadmat(mat_fname) # Read the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "leafLabels = mat_contents['leafLabels']\n",
    "leafLabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Explore this dataset. Can you classify the leaves? Cluster them? Reduce dimension to get a better picture of their distribution? Determine 'within-class variability'? \n",
    "\n",
    "You might have to do something tricky to 'vectorize' the data. E.g., the shapes are not preprocessed to be perfectly aligned like MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
