{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "An important task in unsupervised learning is to *cluster* data. That is, we wish to find groupings of similar data without any knowledge of 'ground truth' labels. Let's explore some methods for doing this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $k$-Means Clustering\n",
    "\n",
    "Suppose that we have a point cloud of data $X = \\{\\vec{x}_1,\\ldots,\\vec{x}_N\\}$ with each $\\vec{x}_j \\in \\mathbb{R}^d$. Our goal is to divide $X$ into $k$ clusters, where $k$ is some positive integer we choose ahead of time. \n",
    "\n",
    "As usual, let's construct some toy data to experiment with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "K = 2 # classes\n",
    "N = 100 # in each class\n",
    "dimension = 2\n",
    "X, y = make_blobs(n_samples=N*K, centers=K, n_features=dimension, random_state=12)\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see visually that our data roughly lies in two clusters. Moreover, we have 'ground truth' labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1],c=y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of illustration, suppose we have no labels and that our data lives in a high dimension that we can't plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to *partition* $X$ into $\\{S_1,\\ldots,S_k\\}$ disjoint nonempty subsets $S_j \\subset X$. Let $\\mu_j$ denote the mean of the points in $S_j$; these are called *cluster centers* in this context. We want our partition to minimize the quantity\n",
    "$$\n",
    "\\sum_{j=1}^k \\sum_{\\vec{x} \\in S_j} \\|\\vec{x} - \\mu_j\\|^2.\n",
    "$$\n",
    "\n",
    "The idea is that the winning partition has the data clustered as tightly as possible around the $k$ means (hence the name of the algorithm). \n",
    "\n",
    "It is not possible to solve for this partition explicitly, so we will search for it iteratively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write code to compute the $k$-means clustering partition. \n",
    "\n",
    "**Spoiler:** This function is built into `scikit-learn`. The point is to build the algorithm ourselves first to understand how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The $k$-Means Clustering Algorithm\n",
    "\n",
    "When writing our algorithm, we'll take the opportunity to demonstrate some more useful `numpy` tricks. These will be pointed out as we go along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Initialize with Random Cluster Centers\n",
    "\n",
    "A useful function for this task is `np.random.choice`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "np.random.choice(10,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_centers(X,K):\n",
    "    return X[np.random.choice(len(X),size=K)]\n",
    "    # Pull out entries of X given by the random choice of K indices\n",
    "\n",
    "# Testing\n",
    "print(cluster_centers(X,2))\n",
    "print(cluster_centers(X,2))\n",
    "print(cluster_centers(X,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2:  Determine Clusters\n",
    "\n",
    "For each point in $X$, we figure out which cluster center is nearest to it. \n",
    "\n",
    "We will employ a useful trick called *numpy broadcasting*. If we apply arithmetic operations to `numpy` arrays of incompatible sizes, numpy broadcasting will make sense of this by 'broadcasting' the smaller array over the larger one. This only works under certain conditions on the sizes, so we have to put some thought into setting it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define arrays to test on\n",
    "A = np.array([[1,2]])\n",
    "B = np.array([[0,0],[1,1],[2,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't make sense mathematical to add these different-sized if we think of them as matrices. On the other hand, `numpy` interprets addition as: 'add the row of `A` to *each* row of `B`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It doesn't make sense to add these arrays mathematically\n",
    "A + B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get even trickier by employing the `np.newaxis` function which takes a 1D array to a 2D array, a 2D array to a 3D array, etc. The way that the function affects the array depends on which 'slot' we use it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define a new test array\n",
    "C = np.array([[1,2],[3,4]])\n",
    "Cnew = C[:,np.newaxis,:]\n",
    "print(Cnew.shape)\n",
    "Cnew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following gives an error. To see the general rules for broadcasting, check here: https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "B+C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, our reshaped array `Cnew` follows the rules to be broadcast with `B`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print((B+Cnew).shape)\n",
    "B+Cnew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define our function. The input is the dataset `X` and a collection of cluster centers `centers` (e.g., the output of `cluster_centers(X,K)`). The output is an array indicating the index of the cluster center to which each element $\\vec{x}_j$ of $X$ belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest(X, centers):\n",
    "    distances = np.linalg.norm(X - centers[:, np.newaxis,:], axis=2)\n",
    "    return np.argmin(distances, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "centers = cluster_centers(X,2)\n",
    "closest(X,centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise \n",
    "\n",
    "Take a moment to dissect the code of the `closest` function. Try to undertand exactly what is going on with the broadcasting procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-6.36068278,  5.02371339],\n",
       "       [-6.71451223,  4.45115953]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-6.36068278,  5.02371339]],\n",
       "\n",
       "       [[-6.71451223,  4.45115953]]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centers[:,np.newaxis,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 200, 2)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X - centers[:,np.newaxis,:]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 200)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(X - centers[:, np.newaxis,:], axis=2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Update Centers\n",
    "\n",
    "Now we define an 'update' function. The input is our dataset `X` and a set of cluster centers `centers`. The output is a new collection of cluster centers, obtained by\n",
    "- partitioning the data according to the input cluster centers,\n",
    "- computing the mean within each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_centers(X, centers):\n",
    "    c = closest(X, centers)\n",
    "    K = len(np.unique(c)) # Determine K by finding the number of labels in c\n",
    "    return np.array([X[c==k].mean(axis=0) for k in range(K)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "new_centers(X,centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Iterate the procedure\n",
    "\n",
    "We can now write our algorithm. We simply iterate the procedure above until the cluster center updates stop moving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kMeans(X, K, max_iter = 10000):\n",
    "    # Initializations\n",
    "    centers = X[np.random.choice(len(X),size=K)]\n",
    "    iteration = 0\n",
    "    Delta = 1\n",
    "    # While loop with a hard limit on number of iterations\n",
    "    while Delta > .001 and iteration < max_iter:\n",
    "        moved = new_centers(X,centers)\n",
    "        Delta = np.linalg.norm( moved - centers )\n",
    "        iteration = iteration+1\n",
    "        centers = moved\n",
    "    print('Iterations to converge: ', iteration)\n",
    "    labels = closest(X,centers)\n",
    "    # Output is a tuple\n",
    "    return centers, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it on our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers, labels = kMeans(X,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "p1 = fig.add_subplot(1,2,1)\n",
    "p1.scatter(X[:,0],X[:,1],c=y)\n",
    "plt.title('Ground Truth')\n",
    "\n",
    "p2 = fig.add_subplot(1,2,2)\n",
    "p2.scatter(X[:,0],X[:,1],c=1-labels) # Use 1-labels so the colors match up\n",
    "p2.scatter(centers[:,0],centers[:,1], marker = '^', c = 'r')\n",
    "plt.title('KMeans Algorithm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks great! Of course, there is not reason that $k$-Means should perfectly replicate 'ground truth' labels if the data is not truly clustered. \n",
    "\n",
    "Some other issues:\n",
    "- This is a randomly-initialized iterative algorithm and there is no guarantee that we find an absolute minimum!\n",
    "- We knew that there should be 2 classes ahead of time. The $k$ in $k$-Means is chosen by the user, so it is definitely possible to make the 'wrong' choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers, labels = kMeans(X,4)\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1],c=labels) \n",
    "plt.scatter(centers[:,0],centers[:,1], marker = '^', c = 'r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Try your $k$-Means algorithm on examples `X1` through `X4` below. For each example, try several values of $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X1, y1 = make_blobs(n_samples=500, center_box=(-3,3), centers=3, random_state=6)\n",
    "\n",
    "\n",
    "X2, y2 = make_blobs(n_samples=1000, centers=4, random_state=1)\n",
    "\n",
    "xs = np.linspace(0,2*np.pi,500)\n",
    "X3 = np.array([np.cos(xs),np.sin(xs)]).T\n",
    "\n",
    "from sklearn.datasets import make_circles\n",
    "X4, y4 = make_circles(n_samples=500, noise = 0.02, random_state = 3)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "p1 = fig.add_subplot(2,2,1)\n",
    "p1.scatter(X1[:,0],X1[:,1])\n",
    "plt.title('Example 1')\n",
    "\n",
    "p2 = fig.add_subplot(2,2,2)\n",
    "p2.scatter(X2[:,0],X2[:,1])\n",
    "plt.title('Example 2')\n",
    "\n",
    "p3 = fig.add_subplot(2,2,3)\n",
    "p3.scatter(X3[:,0],X3[:,1])\n",
    "plt.title('Example 3');\n",
    "\n",
    "p4 = fig.add_subplot(2,2,4)\n",
    "p4.scatter(X4[:,0],X4[:,1])\n",
    "plt.title('Example 4');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = X4\n",
    "\n",
    "centers, labels = kMeans(A,2)\n",
    "\n",
    "plt.scatter(A[:,0],A[:,1],c=labels) \n",
    "plt.scatter(centers[:,0],centers[:,1], marker = '^', c = 'r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $k$-Means with SciKit-Learn\n",
    "\n",
    "As mentioned above, `scikit-learn` has $k$-Means Clustering capability, as well as several useful functions for analyzing the results.\n",
    "\n",
    "Let's first try the `scikit-learn` implementation on our toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=2).fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract labels..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These agree with our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers, labels = kMeans(X,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(centers)\n",
    "print(np.linalg.norm(labels - kmeans.labels_))\n",
    "# Might need to switch to 1-labels above to see the correct result, since we initialize randomly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $k$-Means on MNIST\n",
    "\n",
    "Of course we need to try out the algorithm on MNIST!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "MNIST, MNISTlabels = load_digits(return_X_y=True)\n",
    "# The included option automatically gives us the vectors and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let&rsquo;s cluster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3).fit(MNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the ground truth labels (namely `MNISTlabels`) are known, there are a variety of metrics that one can use to evaluate the quality of clustering. Let's use the \"Adjusted Rand Index\" (see https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html). For this score, random labeling should give something close to 0, perfect labeling gives 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "adjusted_rand_score(MNISTlabels, kmeans.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does a bad job. Of course, 3 clusters is not a good choice for MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Run $k$-Means clustering on MNIST for a range of choices for $k$, compute adjusted rand scores for each choice, then plot the results. What looks like the best choice for $k$? Does that agree with intuition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for j in range(1,25):\n",
    "    kmeans = KMeans(n_clusters=j).fit(MNIST)\n",
    "    scores.append(adjusted_rand_score(MNISTlabels, kmeans.labels_))\n",
    "\n",
    "plt.plot(list(range(1,25)),scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look which digits are getting clustered together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10).fit(MNIST)\n",
    "\n",
    "digit = 7 # Note: label 7 doesn't necessarily correspond to the digit 7. Label names are random!\n",
    "images = MNIST[kmeans.labels_ == digit]\n",
    "\n",
    "fig, axes = plt.subplots(10,10,figsize=(10,10))\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        axes[i,j].axis('off')\n",
    "        axes[i,j].imshow(images[i * 10 + j].reshape(8,8),cmap='gray',interpolation='sinc')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Many Clusters?\n",
    "\n",
    "Since we must choose a value of $k$ to run $k$-Means clustering, there is a big questions when doing *unsupervised learning* (i.e., we don't have 'ground truth' labels): what is the correct choice of $k$?\n",
    "\n",
    "There is no *real* answer, but we can make an educated guess by looking at the *inertia* of the clustering. This is just the value of the function we were optimizing to begin with, evaulated on our clustering partition:\n",
    "$$\n",
    "\\sum_{j=1}^k \\sum_{\\vec{x} \\in S_j} \\|\\vec{x} - \\mu_j\\|^2.\n",
    "$$\n",
    "\n",
    "This is computed in `scikit-learn` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3).fit(X)\n",
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let&rsquo;s plot this within-cluster sum-of-squares for the clusters computed via `KMeans` for multiple choices of `n_clusters`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(1,10)), [KMeans(n_clusters=j).fit(X).inertia_ for j in range(1,10)] )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we increase the number of clusters, the inertia will *always* decrease, so we are not looking for a local min. Instead, we look for an \"elbow\", where the slope of the intertia curve abruptly changes. If such an elbow appears, this is generally accepted to be an optimal number of clusters. For the dataset `X`, this tells us that the optimal number of clusters is 2, which should agree with our intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Run this \"elbow analysis\" on examples `X1` through `X4` from above. Can you determine an optimal number of clusters in each case? What if you run the same sort of analysis on the MNIST dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(1,10)), [KMeans(n_clusters=j).fit(X1).inertia_ for j in range(1,10)] )\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
